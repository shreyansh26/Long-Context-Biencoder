nohup: ignoring input
[2023-08-11 13:25:15,947] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Batch size: 32
Starting data loading...
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
/home/shreyansh/long_context_biencoder_v2/mosaic_bert/bert_layers.py:177: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of BertModel were not initialized from the model checkpoint at mosaicml/mosaic-bert-base-seqlen-1024 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
step 99, avg loss 1.1590538024902344
step 199, avg loss 0.6350905299186707
step 299, avg loss 0.3420591950416565
step 399, avg loss 0.40533870458602905
step 499, avg loss 0.28837254643440247
step 599, avg loss 0.2278306484222412
step 699, avg loss 0.2459784597158432
step 799, avg loss 0.25426483154296875
step 899, avg loss 0.2132532298564911
step 999, avg loss 0.2629692852497101
step 1099, avg loss 0.2010640799999237
step 1199, avg loss 0.2836410701274872
step 1299, avg loss 0.22326074540615082
step 1399, avg loss 0.23540791869163513
step 1499, avg loss 0.19641730189323425
step 1599, avg loss 0.1774968057870865
step 1699, avg loss 0.2825840413570404
step 1799, avg loss 0.251788467168808
step 1899, avg loss 0.18741978704929352
step 1999, avg loss 0.19931712746620178
step 2099, avg loss 0.18888847529888153
step 2199, avg loss 0.1950220763683319
step 2299, avg loss 0.1968635767698288
step 2399, avg loss 0.16372162103652954
step 2499, avg loss 0.3063189387321472
step 2599, avg loss 0.18898862600326538
step 2699, avg loss 0.2007066309452057
step 2799, avg loss 0.22398938238620758
step 2899, avg loss 0.16247718036174774
step 2999, avg loss 0.16689574718475342
step 3099, avg loss 0.2287159413099289
step 3199, avg loss 0.20060250163078308
step 3299, avg loss 0.12436486780643463
step 3399, avg loss 0.21164168417453766
step 3499, avg loss 0.208174929022789
step 3599, avg loss 0.2205192744731903
step 3699, avg loss 0.20166651904582977
step 3799, avg loss 0.18120209872722626
step 3899, avg loss 0.1689424067735672
step 3999, avg loss 0.1515810787677765
step 4099, avg loss 0.1592862606048584
step 4199, avg loss 0.1541041135787964
step 4299, avg loss 0.23633156716823578
step 4399, avg loss 0.22715933620929718
step 4499, avg loss 0.19946463406085968
step 4599, avg loss 0.16388435661792755
step 4699, avg loss 0.1732332557439804
step 4799, avg loss 0.20555830001831055
step 4899, avg loss 0.1439780592918396
step 4999, avg loss 0.16655966639518738
saving model at long_biencoder/5000
step 5099, avg loss 0.229879692196846
step 5199, avg loss 0.18515419960021973
step 5299, avg loss 0.18074922263622284
step 5399, avg loss 0.21065734326839447
step 5499, avg loss 0.21468769013881683
step 5599, avg loss 0.17792674899101257
step 5699, avg loss 0.15016162395477295
step 5799, avg loss 0.24074208736419678
step 5899, avg loss 0.22582653164863586
step 5999, avg loss 0.20415173470973969
step 6099, avg loss 0.19098685681819916
step 6199, avg loss 0.204892098903656
step 6299, avg loss 0.15846094489097595
step 6399, avg loss 0.165841206908226
step 6499, avg loss 0.1253318190574646
step 6599, avg loss 0.2256031632423401
step 6699, avg loss 0.17323289811611176
step 6799, avg loss 0.24649745225906372
step 6899, avg loss 0.17115886509418488
step 6999, avg loss 0.19440904259681702
step 7099, avg loss 0.17440320551395416
step 7199, avg loss 0.1553436815738678
step 7299, avg loss 0.20911423861980438
step 7399, avg loss 0.21583019196987152
step 7499, avg loss 0.16363513469696045
step 7599, avg loss 0.23825904726982117
step 7699, avg loss 0.19380754232406616
step 7799, avg loss 0.1836017221212387
step 7899, avg loss 0.17127318680286407
step 7999, avg loss 0.15201857686042786
step 8099, avg loss 0.14604952931404114
step 8199, avg loss 0.1731693595647812
step 8299, avg loss 0.18100887537002563
step 8399, avg loss 0.15146970748901367
step 8499, avg loss 0.18679237365722656
step 8599, avg loss 0.18041512370109558
step 8699, avg loss 0.15894268453121185
step 8799, avg loss 0.27238190174102783
step 8899, avg loss 0.13501115143299103
step 8999, avg loss 0.1830209344625473
step 9099, avg loss 0.22540618479251862
step 9199, avg loss 0.19787654280662537
step 9299, avg loss 0.17844834923744202
step 9399, avg loss 0.182865172624588
step 9499, avg loss 0.1252121478319168
step 9599, avg loss 0.14433960616588593
step 9699, avg loss 0.16388019919395447
step 9799, avg loss 0.12313536554574966
step 9899, avg loss 0.18897300958633423
step 9999, avg loss 0.22114883363246918
saving model at long_biencoder/10000
step 10099, avg loss 0.13386419415473938
step 10199, avg loss 0.23160146176815033
step 10299, avg loss 0.1446351855993271
step 10399, avg loss 0.22049164772033691
step 10499, avg loss 0.16601166129112244
step 10599, avg loss 0.2109593003988266
step 10699, avg loss 0.1488022804260254
step 10799, avg loss 0.15143264830112457
step 10899, avg loss 0.18147271871566772
step 10999, avg loss 0.1889178454875946
step 11099, avg loss 0.14901916682720184
step 11199, avg loss 0.18970532715320587
step 11299, avg loss 0.1322973519563675
step 11399, avg loss 0.13731619715690613
step 11499, avg loss 0.12616531550884247
step 11599, avg loss 0.1389995962381363
step 11699, avg loss 0.1727108210325241
step 11799, avg loss 0.22356431186199188
step 11899, avg loss 0.18830230832099915
step 11999, avg loss 0.1778811365365982
step 12099, avg loss 0.1464683562517166
step 12199, avg loss 0.1406007707118988
step 12299, avg loss 0.1521613448858261
step 12399, avg loss 0.15677094459533691
step 12499, avg loss 0.22578652203083038
step 12599, avg loss 0.18277719616889954
step 12699, avg loss 0.17722034454345703
step 12799, avg loss 0.16187991201877594
step 12899, avg loss 0.17512111365795135
step 12999, avg loss 0.15937432646751404
step 13099, avg loss 0.1586628258228302
step 13199, avg loss 0.23340457677841187
step 13299, avg loss 0.14169661700725555
step 13399, avg loss 0.20244485139846802
step 13499, avg loss 0.16264167428016663
step 13599, avg loss 0.16334086656570435
step 13699, avg loss 0.18786318600177765
step 13799, avg loss 0.15412722527980804
step 13899, avg loss 0.16046227514743805
step 13999, avg loss 0.16129443049430847
step 14099, avg loss 0.22253401577472687
step 14199, avg loss 0.14330172538757324
step 14299, avg loss 0.1809888482093811
step 14399, avg loss 0.1392984241247177
step 14499, avg loss 0.18481959402561188
step 14599, avg loss 0.12749649584293365
step 14699, avg loss 0.16611149907112122
step 14799, avg loss 0.17785073816776276
step 14899, avg loss 0.17728745937347412
step 14999, avg loss 0.14855726063251495
saving model at long_biencoder/15000
step 15099, avg loss 0.16065679490566254
step 15199, avg loss 0.18729519844055176
step 15299, avg loss 0.20537284016609192
step 15399, avg loss 0.18501533567905426
step 15499, avg loss 0.11744231730699539
step 15599, avg loss 0.1378798633813858
step 15699, avg loss 0.2018101066350937
step 15799, avg loss 0.13470926880836487
step 15899, avg loss 0.13409852981567383
step 15999, avg loss 0.12047973275184631
step 16099, avg loss 0.17951782047748566
step 16199, avg loss 0.18333104252815247
step 16299, avg loss 0.20192763209342957
step 16399, avg loss 0.19220934808254242
step 16499, avg loss 0.1488925665616989
step 16599, avg loss 0.1674576848745346
step 16699, avg loss 0.18478472530841827
step 16799, avg loss 0.1500083953142166
step 16899, avg loss 0.1951686292886734
step 16999, avg loss 0.11306079477071762
step 17099, avg loss 0.21529574692249298
step 17199, avg loss 0.15519598126411438
step 17299, avg loss 0.1547486037015915
step 17399, avg loss 0.12631438672542572
step 17499, avg loss 0.17835509777069092
step 17599, avg loss 0.16080208122730255
step 17699, avg loss 0.1947426199913025
step 17799, avg loss 0.18532082438468933
step 17899, avg loss 0.11450006067752838
step 17999, avg loss 0.15009403228759766
step 18099, avg loss 0.1622997373342514
step 18199, avg loss 0.11047779768705368
step 18299, avg loss 0.11750899255275726
step 18399, avg loss 0.16495828330516815
step 18499, avg loss 0.13893969357013702
step 18599, avg loss 0.13787177205085754
step 18699, avg loss 0.15223611891269684
step 18799, avg loss 0.16486747562885284
step 18899, avg loss 0.1509033739566803
step 18999, avg loss 0.16044652462005615
step 19099, avg loss 0.132240429520607
step 19199, avg loss 0.15452754497528076
step 19299, avg loss 0.20073257386684418
step 19399, avg loss 0.15258124470710754
step 19499, avg loss 0.16592368483543396
step 19599, avg loss 0.17901591956615448
step 19699, avg loss 0.21841184794902802
step 19799, avg loss 0.151109516620636
step 19899, avg loss 0.1648005098104477
step 19999, avg loss 0.10806877911090851
saving model at long_biencoder/20000
step 20099, avg loss 0.14419998228549957
step 20199, avg loss 0.15730826556682587
step 20299, avg loss 0.163455992937088
step 20399, avg loss 0.12369763851165771
step 20499, avg loss 0.18813225626945496
step 20599, avg loss 0.21130242943763733
step 20699, avg loss 0.15297649800777435
step 20799, avg loss 0.19442321360111237
step 20899, avg loss 0.11248864233493805
step 20999, avg loss 0.20975361764431
step 21099, avg loss 0.12481972575187683
step 21199, avg loss 0.12173016369342804
step 21299, avg loss 0.12047120183706284
step 21399, avg loss 0.14605146646499634
step 21499, avg loss 0.13254889845848083
step 21599, avg loss 0.1379794329404831
step 21699, avg loss 0.15565739572048187
step 21799, avg loss 0.1675536334514618
step 21899, avg loss 0.14266157150268555
step 21999, avg loss 0.16879799962043762
step 22099, avg loss 0.14867088198661804
step 22199, avg loss 0.16165228188037872
step 22299, avg loss 0.11808372288942337
step 22399, avg loss 0.21143151819705963
step 22499, avg loss 0.13905303180217743
step 22599, avg loss 0.1597411334514618
step 22699, avg loss 0.1474279910326004
step 22799, avg loss 0.15106701850891113
step 22899, avg loss 0.17345905303955078
step 22999, avg loss 0.11763547360897064
step 23099, avg loss 0.1446995735168457
step 23199, avg loss 0.11134389787912369
step 23299, avg loss 0.1886577159166336
step 23399, avg loss 0.1488891988992691
step 23499, avg loss 0.1970590502023697
step 23599, avg loss 0.16901861131191254
step 23699, avg loss 0.1839592456817627
step 23799, avg loss 0.11115702986717224
step 23899, avg loss 0.1723206490278244
step 23999, avg loss 0.1349954903125763
step 24099, avg loss 0.16559343039989471
step 24199, avg loss 0.14484310150146484
step 24299, avg loss 0.1019560918211937
step 24399, avg loss 0.24617746472358704
step 24499, avg loss 0.2144702672958374
step 24599, avg loss 0.14981460571289062
step 24699, avg loss 0.13783912360668182
step 24799, avg loss 0.1294143795967102
step 24899, avg loss 0.18506261706352234
step 24999, avg loss 0.1431109607219696
saving model at long_biencoder/25000
step 25099, avg loss 0.1397469937801361
step 25199, avg loss 0.13265149295330048
step 25299, avg loss 0.15563499927520752
step 25399, avg loss 0.12316930294036865
step 25499, avg loss 0.12599585950374603
step 25599, avg loss 0.11972867697477341
step 25699, avg loss 0.17941272258758545
step 25799, avg loss 0.1399531364440918
step 25899, avg loss 0.12703095376491547
step 25999, avg loss 0.14358963072299957
step 26099, avg loss 0.11795175075531006
step 26199, avg loss 0.16011148691177368
step 26299, avg loss 0.11945145577192307
step 26399, avg loss 0.12075187265872955
step 26499, avg loss 0.12161033600568771
step 26599, avg loss 0.13143722712993622
step 26699, avg loss 0.12131787836551666
step 26799, avg loss 0.16570857167243958
step 26899, avg loss 0.16770213842391968
step 26999, avg loss 0.15654553472995758
step 27099, avg loss 0.14955514669418335
step 27199, avg loss 0.1217004656791687
step 27299, avg loss 0.1451660841703415
step 27399, avg loss 0.11806866526603699
step 27499, avg loss 0.12791353464126587
step 27599, avg loss 0.16734184324741364
step 27699, avg loss 0.12323629856109619
step 27799, avg loss 0.1853407621383667
step 27899, avg loss 0.17305906116962433
step 27999, avg loss 0.14527156949043274
step 28099, avg loss 0.14978763461112976
step 28199, avg loss 0.1326671689748764
step 28299, avg loss 0.1537252813577652
step 28399, avg loss 0.11050812155008316
step 28499, avg loss 0.1188536211848259
step 28599, avg loss 0.17805831134319305
step 28699, avg loss 0.1496797502040863
step 28799, avg loss 0.20055171847343445
step 28899, avg loss 0.1481524109840393
step 28999, avg loss 0.1568923145532608
step 29099, avg loss 0.16954301297664642
step 29199, avg loss 0.21041730046272278
step 29299, avg loss 0.15633460879325867
step 29399, avg loss 0.097731813788414
step 29499, avg loss 0.14076952636241913
step 29599, avg loss 0.14772221446037292
step 29699, avg loss 0.14688868820667267
step 29799, avg loss 0.15365353226661682
step 29899, avg loss 0.17909610271453857
step 29999, avg loss 0.15996332466602325
saving model at long_biencoder/30000
step 30099, avg loss 0.22478602826595306
step 30199, avg loss 0.16011859476566315
step 30299, avg loss 0.14650380611419678
step 30399, avg loss 0.1712200939655304
step 30499, avg loss 0.127285435795784
step 30599, avg loss 0.1465621292591095
step 30699, avg loss 0.15317484736442566
step 30799, avg loss 0.10955041646957397
step 30899, avg loss 0.12053363770246506
step 30999, avg loss 0.12483308464288712
step 31099, avg loss 0.15288668870925903
step 31199, avg loss 0.16225427389144897
step 31299, avg loss 0.12631618976593018
step 31399, avg loss 0.15212121605873108
step 31499, avg loss 0.14312106370925903
step 31599, avg loss 0.1260259449481964
step 31699, avg loss 0.12304258346557617
step 31799, avg loss 0.13158370554447174
step 31899, avg loss 0.13150790333747864
step 31999, avg loss 0.13232462108135223
step 32099, avg loss 0.10386853665113449
step 32199, avg loss 0.1762818992137909
step 32299, avg loss 0.16260264813899994
step 32399, avg loss 0.14990724623203278
step 32499, avg loss 0.1544271856546402
step 32599, avg loss 0.11270078271627426
step 32699, avg loss 0.14707258343696594
step 32799, avg loss 0.12910856306552887
step 32899, avg loss 0.14139492809772491
step 32999, avg loss 0.1530005931854248
step 33099, avg loss 0.16611981391906738
step 33199, avg loss 0.13085030019283295
step 33299, avg loss 0.15145835280418396
step 33399, avg loss 0.1054319515824318
step 33499, avg loss 0.15597745776176453
step 33599, avg loss 0.1337503045797348
step 33699, avg loss 0.16643717885017395
step 33799, avg loss 0.12715578079223633
step 33899, avg loss 0.11724860966205597
step 33999, avg loss 0.15776818990707397
step 34099, avg loss 0.16526883840560913
step 34199, avg loss 0.11197860538959503
step 34299, avg loss 0.14182355999946594
step 34399, avg loss 0.12346244603395462
step 34499, avg loss 0.1345978081226349
step 34599, avg loss 0.12338854372501373
step 34699, avg loss 0.13765624165534973
step 34799, avg loss 0.13668298721313477
step 34899, avg loss 0.12200891971588135
step 34999, avg loss 0.11964624375104904
saving model at long_biencoder/35000
step 35099, avg loss 0.10722699016332626
step 35199, avg loss 0.16695086658000946
step 35299, avg loss 0.13554203510284424
step 35399, avg loss 0.10391644388437271
step 35499, avg loss 0.1177968978881836
step 35599, avg loss 0.16657796502113342
step 35699, avg loss 0.13445454835891724
step 35799, avg loss 0.12202964723110199
step 35899, avg loss 0.09735142439603806
step 35999, avg loss 0.1672717034816742
step 36099, avg loss 0.10590042173862457
step 36199, avg loss 0.13778270781040192
step 36299, avg loss 0.13289253413677216
step 36399, avg loss 0.13625577092170715
step 36499, avg loss 0.18917521834373474
step 36599, avg loss 0.13666939735412598
step 36699, avg loss 0.12541955709457397
step 36799, avg loss 0.1817466914653778
step 36899, avg loss 0.1227983608841896
step 36999, avg loss 0.13126856088638306
step 37099, avg loss 0.10007904469966888
step 37199, avg loss 0.11767107993364334
step 37299, avg loss 0.11359819024801254
step 37399, avg loss 0.141608327627182
step 37499, avg loss 0.11761131137609482
step 37599, avg loss 0.13825348019599915
step 37699, avg loss 0.12325121462345123
step 37799, avg loss 0.17214947938919067
step 37899, avg loss 0.138311967253685
step 37999, avg loss 0.13189946115016937
step 38099, avg loss 0.12828344106674194
step 38199, avg loss 0.18191058933734894
step 38299, avg loss 0.18843558430671692
step 38399, avg loss 0.15594196319580078
step 38499, avg loss 0.1506325900554657
step 38599, avg loss 0.16172364354133606
step 38699, avg loss 0.1300881803035736
step 38799, avg loss 0.12095115333795547
step 38899, avg loss 0.14905358850955963
step 38999, avg loss 0.13165231049060822
step 39099, avg loss 0.11364160478115082
step 39199, avg loss 0.10803709924221039
step 39299, avg loss 0.14322105050086975
step 39399, avg loss 0.10980270802974701
step 39499, avg loss 0.1211855486035347
step 39599, avg loss 0.14381523430347443
step 39699, avg loss 0.1163712665438652
step 39799, avg loss 0.1665036678314209
step 39899, avg loss 0.12795427441596985
step 39999, avg loss 0.12142755091190338
saving model at long_biencoder/40000
step 40099, avg loss 0.1592138409614563
step 40199, avg loss 0.15549243986606598
step 40299, avg loss 0.12052889913320541
step 40399, avg loss 0.12968800961971283
step 40499, avg loss 0.15654537081718445
step 40599, avg loss 0.1607024222612381
step 40699, avg loss 0.12238214910030365
step 40799, avg loss 0.16457988321781158
step 40899, avg loss 0.1852336972951889
step 40999, avg loss 0.1363935023546219
step 41099, avg loss 0.11804424226284027
step 41199, avg loss 0.14025932550430298
step 41299, avg loss 0.14394249022006989
step 41399, avg loss 0.13259251415729523
step 41499, avg loss 0.12506890296936035
step 41599, avg loss 0.12987099587917328
step 41699, avg loss 0.17194536328315735
step 41799, avg loss 0.12889885902404785
step 41899, avg loss 0.14806325733661652
step 41999, avg loss 0.13899725675582886
step 42099, avg loss 0.13079418241977692
step 42199, avg loss 0.10128342360258102
step 42299, avg loss 0.13708829879760742
step 42399, avg loss 0.19487741589546204
step 42499, avg loss 0.13111872971057892
step 42599, avg loss 0.1096554696559906
step 42699, avg loss 0.13941964507102966
step 42799, avg loss 0.1372908502817154
step 42899, avg loss 0.1684093177318573
step 42999, avg loss 0.10958758741617203
step 43099, avg loss 0.14542578160762787
step 43199, avg loss 0.13169893622398376
step 43299, avg loss 0.11514593660831451
step 43399, avg loss 0.10755790770053864
step 43499, avg loss 0.1414700150489807
step 43599, avg loss 0.13853368163108826
step 43699, avg loss 0.14210575819015503
step 43799, avg loss 0.15079651772975922
step 43899, avg loss 0.12088610976934433
step 43999, avg loss 0.15575310587882996
step 44099, avg loss 0.15644685924053192
step 44199, avg loss 0.11846549063920975
step 44299, avg loss 0.1578313410282135
step 44399, avg loss 0.1552550494670868
step 44499, avg loss 0.14176423847675323
step 44599, avg loss 0.09399385005235672
step 44699, avg loss 0.12303285300731659
step 44799, avg loss 0.12178350239992142
step 44899, avg loss 0.16268590092658997
step 44999, avg loss 0.16949643194675446
saving model at long_biencoder/45000
step 45099, avg loss 0.10552085936069489
step 45199, avg loss 0.13022489845752716
step 45299, avg loss 0.13219374418258667
step 45399, avg loss 0.17045354843139648
step 45499, avg loss 0.15233954787254333
step 45599, avg loss 0.14414872229099274
step 45699, avg loss 0.09948458522558212
step 45799, avg loss 0.2170938402414322
step 45899, avg loss 0.1432943493127823
step 45999, avg loss 0.09381838887929916
step 46099, avg loss 0.14708851277828217
step 46199, avg loss 0.136163130402565
step 46299, avg loss 0.11331542581319809
step 46399, avg loss 0.14319269359111786
step 46499, avg loss 0.11190276592969894
step 46599, avg loss 0.1625860333442688
step 46699, avg loss 0.10063035041093826
step 46799, avg loss 0.15734510123729706
step 46899, avg loss 0.15824566781520844
step 46999, avg loss 0.1198076605796814
step 47099, avg loss 0.1411931812763214
step 47199, avg loss 0.1360992044210434
step 47299, avg loss 0.1491573601961136
step 47399, avg loss 0.16553322970867157
step 47499, avg loss 0.11737491935491562
step 47599, avg loss 0.09822550415992737
step 47699, avg loss 0.12055207788944244
step 47799, avg loss 0.17289121448993683
step 47899, avg loss 0.16340751945972443
step 47999, avg loss 0.16641591489315033
step 48099, avg loss 0.134249746799469
step 48199, avg loss 0.10276893526315689
step 48299, avg loss 0.11694969981908798
step 48399, avg loss 0.2093893438577652
step 48499, avg loss 0.0992528423666954
step 48599, avg loss 0.09746246039867401
step 48699, avg loss 0.10455628484487534
step 48799, avg loss 0.11904036998748779
step 48899, avg loss 0.1423676609992981
step 48999, avg loss 0.17615175247192383
step 49099, avg loss 0.15640763938426971
step 49199, avg loss 0.13000303506851196
step 49299, avg loss 0.14129547774791718
step 49399, avg loss 0.1080486923456192
step 49499, avg loss 0.11723604798316956
step 49599, avg loss 0.15797029435634613
step 49699, avg loss 0.10345035046339035
step 49799, avg loss 0.13818691670894623
step 49899, avg loss 0.12651993334293365
step 49999, avg loss 0.15631617605686188
saving model at long_biencoder/50000
step 50099, avg loss 0.1582261174917221
step 50199, avg loss 0.15085361897945404
step 50299, avg loss 0.15187916159629822
step 50399, avg loss 0.1188964694738388
step 50499, avg loss 0.1294180154800415
step 50599, avg loss 0.09497798979282379
step 50699, avg loss 0.11925831437110901
step 50799, avg loss 0.12554886937141418
step 50899, avg loss 0.1382356733083725
step 50999, avg loss 0.12121471017599106
step 51099, avg loss 0.15569432079792023
step 51199, avg loss 0.10920055210590363
step 51299, avg loss 0.11463755369186401
step 51399, avg loss 0.11451467871665955
step 51499, avg loss 0.1699163168668747
step 51599, avg loss 0.12973864376544952
step 51699, avg loss 0.13250738382339478
step 51799, avg loss 0.1341496706008911
step 51899, avg loss 0.13427461683750153
step 51999, avg loss 0.10907287150621414
step 52099, avg loss 0.16587059199810028
step 52199, avg loss 0.10649852454662323
step 52299, avg loss 0.0969696193933487
step 52399, avg loss 0.15917956829071045
step 52499, avg loss 0.08928773552179337
step 52599, avg loss 0.1623668074607849
step 52699, avg loss 0.12249963730573654
step 52799, avg loss 0.08307080715894699
step 52899, avg loss 0.20292013883590698
step 52999, avg loss 0.10500254482030869
step 53099, avg loss 0.11565639823675156
step 53199, avg loss 0.15418674051761627
step 53299, avg loss 0.1306259036064148
step 53399, avg loss 0.19060327112674713
step 53499, avg loss 0.11568213254213333
step 53599, avg loss 0.17777206003665924
step 53699, avg loss 0.173466295003891
step 53799, avg loss 0.16255883872509003
step 53899, avg loss 0.1133929118514061
step 53999, avg loss 0.11241269111633301
step 54099, avg loss 0.13582532107830048
step 54199, avg loss 0.12229248136281967
step 54299, avg loss 0.11448914557695389
step 54399, avg loss 0.10543986409902573
step 54499, avg loss 0.11661958694458008
step 54599, avg loss 0.1770087629556656
step 54699, avg loss 0.12973617017269135
step 54799, avg loss 0.1352710872888565
step 54899, avg loss 0.11494825035333633
step 54999, avg loss 0.13627076148986816
saving model at long_biencoder/55000
step 55099, avg loss 0.10725760459899902
step 55199, avg loss 0.13381169736385345
step 55299, avg loss 0.1210293099284172
step 55399, avg loss 0.1413489133119583
step 55499, avg loss 0.16998685896396637
step 55599, avg loss 0.10535842925310135
step 55699, avg loss 0.1536187082529068
step 55799, avg loss 0.13935332000255585
step 55899, avg loss 0.11660431325435638
step 55999, avg loss 0.15515227615833282
step 56099, avg loss 0.13670714199543
step 56199, avg loss 0.10100191831588745
step 56299, avg loss 0.12899121642112732
step 56399, avg loss 0.1347932070493698
step 56499, avg loss 0.16605766117572784
step 56599, avg loss 0.11605110764503479
step 56699, avg loss 0.0998803973197937
step 56799, avg loss 0.10627604275941849
step 56899, avg loss 0.14954940974712372
step 56999, avg loss 0.12596507370471954
step 57099, avg loss 0.08041036128997803
step 57199, avg loss 0.0916757732629776
step 57299, avg loss 0.1474258154630661
step 57399, avg loss 0.08140966296195984
step 57499, avg loss 0.08363231271505356
step 57599, avg loss 0.14558717608451843
step 57699, avg loss 0.1351345181465149
step 57799, avg loss 0.13278989493846893
step 57899, avg loss 0.15360088646411896
step 57999, avg loss 0.13015668094158173
step 58099, avg loss 0.14143818616867065
step 58199, avg loss 0.11581771820783615
step 58299, avg loss 0.10127245634794235
step 58399, avg loss 0.10973934829235077
step 58499, avg loss 0.12451746314764023
step 58599, avg loss 0.08963876217603683
step 58699, avg loss 0.10829635709524155
step 58799, avg loss 0.0989200696349144
step 58899, avg loss 0.1472763568162918
step 58999, avg loss 0.15774409472942352
step 59099, avg loss 0.17371803522109985
step 59199, avg loss 0.14207278192043304
step 59299, avg loss 0.14025674760341644
step 59399, avg loss 0.19334964454174042
step 59499, avg loss 0.14944638311862946
step 59599, avg loss 0.12415487319231033
step 59699, avg loss 0.13128994405269623
step 59799, avg loss 0.12684999406337738
step 59899, avg loss 0.19861243665218353
step 59999, avg loss 0.12226694822311401
saving model at long_biencoder/60000
step 60099, avg loss 0.19556455314159393
step 60199, avg loss 0.13833929598331451
step 60299, avg loss 0.11182110756635666
step 60399, avg loss 0.11759531497955322
step 60499, avg loss 0.1598035842180252
step 60599, avg loss 0.12432654947042465
step 60699, avg loss 0.13163317739963531
step 60799, avg loss 0.1654340773820877
step 60899, avg loss 0.18799979984760284
step 60999, avg loss 0.1722344011068344
step 61099, avg loss 0.12188104540109634
step 61199, avg loss 0.12800233066082
step 61299, avg loss 0.11246269196271896
step 61399, avg loss 0.15829826891422272
step 61499, avg loss 0.11986439675092697
step 61599, avg loss 0.12304198741912842
step 61699, avg loss 0.1397654265165329
step 61799, avg loss 0.12195467948913574
step 61899, avg loss 0.19292110204696655
step 61999, avg loss 0.14914901554584503
step 62099, avg loss 0.12366930395364761
step 62199, avg loss 0.1597764790058136
step 62299, avg loss 0.1545986533164978
step 62399, avg loss 0.13030773401260376
step 62499, avg loss 0.12205786257982254
step 62599, avg loss 0.10773003101348877
step 62699, avg loss 0.13159723579883575
step 62799, avg loss 0.09713888168334961
step 62899, avg loss 0.1302466094493866
step 62999, avg loss 0.1376318782567978
step 63099, avg loss 0.13681674003601074
step 63199, avg loss 0.18377086520195007
step 63299, avg loss 0.11309219151735306
step 63399, avg loss 0.1439257562160492
step 63499, avg loss 0.17374436557292938
step 63599, avg loss 0.12683652341365814
step 63699, avg loss 0.10901512205600739
step 63799, avg loss 0.09088711440563202
step 63899, avg loss 0.11111051589250565
step 63999, avg loss 0.1256098598241806
step 64099, avg loss 0.12204894423484802
step 64199, avg loss 0.10890611261129379
step 64299, avg loss 0.13047316670417786
step 64399, avg loss 0.11443299800157547
step 64499, avg loss 0.11930455267429352
step 64599, avg loss 0.1639096587896347
step 64699, avg loss 0.15078726410865784
step 64799, avg loss 0.14771990478038788
step 64899, avg loss 0.10129063576459885
step 64999, avg loss 0.1225116103887558
saving model at long_biencoder/65000
step 65099, avg loss 0.15143418312072754
step 65199, avg loss 0.13460631668567657
step 65299, avg loss 0.11415281891822815
step 65399, avg loss 0.14230003952980042
step 65499, avg loss 0.13393962383270264
step 65599, avg loss 0.13073444366455078
step 65699, avg loss 0.1436784267425537
step 65799, avg loss 0.08588188141584396
step 65899, avg loss 0.12579038739204407
step 65999, avg loss 0.13631761074066162
step 66099, avg loss 0.14437218010425568
step 66199, avg loss 0.13361729681491852
step 66299, avg loss 0.15509676933288574
step 66399, avg loss 0.12725836038589478
step 66499, avg loss 0.16834987699985504
step 66599, avg loss 0.1252507120370865
step 66699, avg loss 0.17187249660491943
step 66799, avg loss 0.12301667034626007
step 66899, avg loss 0.13629791140556335
step 66999, avg loss 0.10764651000499725
step 67099, avg loss 0.1528514176607132
step 67199, avg loss 0.1473405659198761
step 67299, avg loss 0.12377599626779556
step 67399, avg loss 0.12738732993602753
step 67499, avg loss 0.11285165697336197
step 67599, avg loss 0.12475376576185226
step 67699, avg loss 0.09921146929264069
step 67799, avg loss 0.13888658583164215
step 67899, avg loss 0.13111838698387146
step 67999, avg loss 0.11133083701133728
step 68099, avg loss 0.11342109739780426
step 68199, avg loss 0.13599643111228943
step 68299, avg loss 0.12429708242416382
step 68399, avg loss 0.14158159494400024
step 68499, avg loss 0.13559430837631226
step 68599, avg loss 0.12863191962242126
step 68699, avg loss 0.08538757264614105
step 68799, avg loss 0.12078014016151428
step 68899, avg loss 0.10213816165924072
step 68999, avg loss 0.17037054896354675
step 69099, avg loss 0.12245922535657883
step 69199, avg loss 0.17220254242420197
step 69299, avg loss 0.15675979852676392
step 69399, avg loss 0.11263617873191833
step 69499, avg loss 0.10608884692192078
step 69599, avg loss 0.12065088748931885
step 69699, avg loss 0.11223407834768295
step 69799, avg loss 0.09995263814926147
step 69899, avg loss 0.10401780158281326
step 69999, avg loss 0.13862934708595276
saving model at long_biencoder/70000
step 70099, avg loss 0.09700176119804382
step 70199, avg loss 0.15623924136161804
step 70299, avg loss 0.1050713062286377
step 70399, avg loss 0.1294984370470047
step 70499, avg loss 0.1768764853477478
step 70599, avg loss 0.1368905007839203
step 70699, avg loss 0.14681880176067352
step 70799, avg loss 0.1441417783498764
step 70899, avg loss 0.1338767260313034
step 70999, avg loss 0.12300179153680801
step 71099, avg loss 0.13056538999080658
step 71199, avg loss 0.09897348284721375
step 71299, avg loss 0.13573114573955536
step 71399, avg loss 0.10936741530895233
step 71499, avg loss 0.12653294205665588
step 71599, avg loss 0.11090221256017685
step 71699, avg loss 0.15184268355369568
step 71799, avg loss 0.11951453983783722
step 71899, avg loss 0.10823690891265869
step 71999, avg loss 0.10780530422925949
step 72099, avg loss 0.11524258553981781
step 72199, avg loss 0.14585773646831512
step 72299, avg loss 0.18089161813259125
step 72399, avg loss 0.07835355401039124
step 72499, avg loss 0.1203794851899147
step 72599, avg loss 0.10116313397884369
step 72699, avg loss 0.15342995524406433
step 72799, avg loss 0.14948926866054535
step 72899, avg loss 0.10428866744041443
step 72999, avg loss 0.12321793287992477
step 73099, avg loss 0.1335119754076004
step 73199, avg loss 0.11758693307638168
step 73299, avg loss 0.11706629395484924
step 73399, avg loss 0.16079279780387878
step 73499, avg loss 0.10208766907453537
step 73599, avg loss 0.10866022855043411
step 73699, avg loss 0.13471555709838867
step 73799, avg loss 0.16398921608924866
step 73899, avg loss 0.1349257081747055
step 73999, avg loss 0.11314769834280014
step 74099, avg loss 0.14058168232440948
step 74199, avg loss 0.09143788367509842
step 74299, avg loss 0.15123070776462555
step 74399, avg loss 0.13122189044952393
step 74499, avg loss 0.14747793972492218
step 74599, avg loss 0.09983357042074203
step 74699, avg loss 0.13825146853923798
step 74799, avg loss 0.0998859852552414
step 74899, avg loss 0.143906831741333
step 74999, avg loss 0.12610894441604614
saving model at long_biencoder/75000
step 75099, avg loss 0.13551385700702667
step 75199, avg loss 0.14254413545131683
step 75299, avg loss 0.10878266394138336
step 75399, avg loss 0.16143065690994263
step 75499, avg loss 0.15536801517009735
step 75599, avg loss 0.180276021361351
step 75699, avg loss 0.09641240537166595
step 75799, avg loss 0.13765881955623627
step 75899, avg loss 0.11470101028680801
step 75999, avg loss 0.11762132495641708
step 76099, avg loss 0.1277967244386673
step 76199, avg loss 0.16919048130512238
step 76299, avg loss 0.14028307795524597
step 76399, avg loss 0.12310818582773209
step 76499, avg loss 0.0948595404624939
step 76599, avg loss 0.10085839778184891
step 76699, avg loss 0.09217899292707443
step 76799, avg loss 0.16353347897529602
step 76899, avg loss 0.16799762845039368
step 76999, avg loss 0.09915569424629211
step 77099, avg loss 0.14679329097270966
step 77199, avg loss 0.10165392607450485
step 77299, avg loss 0.10367714613676071
step 77399, avg loss 0.12273165583610535
step 77499, avg loss 0.09770060330629349
step 77599, avg loss 0.11353269964456558
step 77699, avg loss 0.09996708482503891
step 77799, avg loss 0.13449926674365997
step 77899, avg loss 0.11957991123199463
step 77999, avg loss 0.09886592626571655
step 78099, avg loss 0.19011807441711426
step 78199, avg loss 0.1337386518716812
step 78299, avg loss 0.13109488785266876
step 78399, avg loss 0.11326751112937927
step 78499, avg loss 0.10630962252616882
step 78599, avg loss 0.14523376524448395
step 78699, avg loss 0.1331188976764679
step 78799, avg loss 0.11446946114301682
step 78899, avg loss 0.0878445953130722
step 78999, avg loss 0.10480422526597977
step 79099, avg loss 0.13980506360530853
step 79199, avg loss 0.16580083966255188
step 79299, avg loss 0.11098616570234299
step 79399, avg loss 0.1424328237771988
step 79499, avg loss 0.10830570757389069
step 79599, avg loss 0.14230363070964813
step 79699, avg loss 0.15326382219791412
step 79799, avg loss 0.09676725417375565
step 79899, avg loss 0.11327546834945679
step 79999, avg loss 0.12734942138195038
saving model at long_biencoder/80000
step 80099, avg loss 0.13441184163093567
step 80199, avg loss 0.13233411312103271
step 80299, avg loss 0.11908652633428574
step 80399, avg loss 0.13364849984645844
step 80499, avg loss 0.10946399718523026
step 80599, avg loss 0.10127006471157074
step 80699, avg loss 0.10571187734603882
step 80799, avg loss 0.12242225557565689
step 80899, avg loss 0.18671566247940063
step 80999, avg loss 0.0878654271364212
step 81099, avg loss 0.12204140424728394
step 81199, avg loss 0.09865009784698486
step 81299, avg loss 0.13815408945083618
step 81399, avg loss 0.0943571999669075
step 81499, avg loss 0.12667511403560638
step 81599, avg loss 0.11224500089883804
step 81699, avg loss 0.14261391758918762
step 81799, avg loss 0.13323718309402466
step 81899, avg loss 0.08851563930511475
step 81999, avg loss 0.11415982246398926
step 82099, avg loss 0.10413893312215805
step 82199, avg loss 0.11732220649719238
step 82299, avg loss 0.16592907905578613
step 82399, avg loss 0.13911406695842743
step 82499, avg loss 0.11550664901733398
step 82599, avg loss 0.09475294500589371
step 82699, avg loss 0.12344659864902496
step 82799, avg loss 0.1403367817401886
step 82899, avg loss 0.12428012490272522
step 82999, avg loss 0.14885640144348145
step 83099, avg loss 0.1383218616247177
step 83199, avg loss 0.10443874448537827
step 83299, avg loss 0.1355554610490799
step 83399, avg loss 0.09298980236053467
step 83499, avg loss 0.1421721875667572
step 83599, avg loss 0.1214335635304451
step 83699, avg loss 0.1416751742362976
step 83799, avg loss 0.1338406205177307
step 83899, avg loss 0.1222924217581749
step 83999, avg loss 0.11242946982383728
step 84099, avg loss 0.1252826601266861
step 84199, avg loss 0.11524112522602081
step 84299, avg loss 0.1014954224228859
step 84399, avg loss 0.13162662088871002
step 84499, avg loss 0.17417030036449432
step 84599, avg loss 0.13896353542804718
step 84699, avg loss 0.08947661519050598
step 84799, avg loss 0.1453721672296524
step 84899, avg loss 0.143711656332016
step 84999, avg loss 0.11491483449935913
saving model at long_biencoder/85000
step 85099, avg loss 0.1193326935172081
step 85199, avg loss 0.12621770799160004
step 85299, avg loss 0.09170284867286682
step 85399, avg loss 0.11401490122079849
step 85499, avg loss 0.13709205389022827
step 85599, avg loss 0.0908120647072792
step 85699, avg loss 0.17511633038520813
step 85799, avg loss 0.09091494232416153
step 85899, avg loss 0.12349923700094223
step 85999, avg loss 0.12102706730365753
step 86099, avg loss 0.13000772893428802
step 86199, avg loss 0.13193944096565247
step 86299, avg loss 0.13777542114257812
step 86399, avg loss 0.15068179368972778
step 86499, avg loss 0.09515956044197083
step 86599, avg loss 0.1246076300740242
step 86699, avg loss 0.10932037979364395
step 86799, avg loss 0.13261686265468597
step 86899, avg loss 0.11979594826698303
step 86999, avg loss 0.09798870980739594
step 87099, avg loss 0.10450833290815353
step 87199, avg loss 0.0972205176949501
step 87299, avg loss 0.12092379480600357
step 87399, avg loss 0.15363509953022003
step 87499, avg loss 0.15380477905273438
step 87599, avg loss 0.12790966033935547
step 87699, avg loss 0.10413792729377747
step 87799, avg loss 0.1477290242910385
step 87899, avg loss 0.1660635769367218
step 87999, avg loss 0.13312076032161713
step 88099, avg loss 0.10188235342502594
step 88199, avg loss 0.09371840208768845
step 88299, avg loss 0.16473394632339478
step 88399, avg loss 0.1090778261423111
step 88499, avg loss 0.152131125330925
step 88599, avg loss 0.12293197214603424
step 88699, avg loss 0.14017672836780548
step 88799, avg loss 0.08273991197347641
step 88899, avg loss 0.1266220510005951
step 88999, avg loss 0.1287555992603302
step 89099, avg loss 0.09659048169851303
step 89199, avg loss 0.13004815578460693
step 89299, avg loss 0.13268719613552094
step 89399, avg loss 0.12820883095264435
step 89499, avg loss 0.10363460332155228
step 89599, avg loss 0.1119137555360794
step 89699, avg loss 0.1220482811331749
step 89799, avg loss 0.11437074095010757
step 89899, avg loss 0.14048251509666443
step 89999, avg loss 0.09202177822589874
saving model at long_biencoder/90000
step 90099, avg loss 0.11939594149589539
step 90199, avg loss 0.1309279501438141
step 90299, avg loss 0.1265527755022049
step 90399, avg loss 0.14377188682556152
step 90499, avg loss 0.11551516503095627
step 90599, avg loss 0.0701666995882988
step 90699, avg loss 0.1034460961818695
step 90799, avg loss 0.138446643948555
step 90899, avg loss 0.10771592706441879
step 90999, avg loss 0.11210884898900986
step 91099, avg loss 0.13449783623218536
step 91199, avg loss 0.12372102588415146
step 91299, avg loss 0.11141086369752884
step 91399, avg loss 0.09456153959035873
step 91499, avg loss 0.15499739348888397
step 91599, avg loss 0.11878116428852081
step 91699, avg loss 0.11640845239162445
step 91799, avg loss 0.14973822236061096
step 91899, avg loss 0.1492323875427246
step 91999, avg loss 0.10043825209140778
step 92099, avg loss 0.14532622694969177
step 92199, avg loss 0.10708378255367279
step 92299, avg loss 0.09216531366109848
step 92399, avg loss 0.1419723480939865
step 92499, avg loss 0.10085847973823547
step 92599, avg loss 0.09339970350265503
step 92699, avg loss 0.10882902890443802
step 92799, avg loss 0.09454188495874405
step 92899, avg loss 0.12744006514549255
step 92999, avg loss 0.11615461856126785
step 93099, avg loss 0.13067248463630676
step 93199, avg loss 0.11257308721542358
step 93299, avg loss 0.1619967222213745
step 93399, avg loss 0.10662174224853516
step 93499, avg loss 0.15677234530448914
step 93599, avg loss 0.1023361086845398
step 93699, avg loss 0.13823336362838745
step 93799, avg loss 0.11867519468069077
step 93899, avg loss 0.08393489569425583
step 93999, avg loss 0.12849140167236328
step 94099, avg loss 0.11743161827325821
step 94199, avg loss 0.11861147731542587
step 94299, avg loss 0.1167847290635109
step 94399, avg loss 0.122176393866539
step 94499, avg loss 0.11225313693284988
step 94599, avg loss 0.17455676198005676
step 94699, avg loss 0.11195184290409088
step 94799, avg loss 0.09221301227807999
step 94899, avg loss 0.09086360782384872
step 94999, avg loss 0.17485195398330688
saving model at long_biencoder/95000
step 95099, avg loss 0.13605132699012756
step 95199, avg loss 0.12090424448251724
step 95299, avg loss 0.09351417422294617
step 95399, avg loss 0.1481676697731018
step 95499, avg loss 0.0938623920083046
step 95599, avg loss 0.1590695083141327
step 95699, avg loss 0.10836677998304367
step 95799, avg loss 0.0984957218170166
step 95899, avg loss 0.1081710234284401
step 95999, avg loss 0.10614003986120224
step 96099, avg loss 0.1532677710056305
step 96199, avg loss 0.1113329604268074
step 96299, avg loss 0.08246595412492752
step 96399, avg loss 0.1360226720571518
step 96499, avg loss 0.10357341170310974
step 96599, avg loss 0.1323060244321823
step 96699, avg loss 0.120888851583004
step 96799, avg loss 0.132183238863945
step 96899, avg loss 0.11781764030456543
step 96999, avg loss 0.1426488757133484
step 97099, avg loss 0.10903965681791306
step 97199, avg loss 0.11165882647037506
step 97299, avg loss 0.14231565594673157
step 97399, avg loss 0.13534632325172424
step 97499, avg loss 0.1300196647644043
step 97599, avg loss 0.10101765394210815
step 97699, avg loss 0.14317388832569122
step 97799, avg loss 0.09232580661773682
step 97899, avg loss 0.11634983122348785
step 97999, avg loss 0.10435446351766586
step 98099, avg loss 0.09240822494029999
step 98199, avg loss 0.12181714922189713
step 98299, avg loss 0.09020226448774338
step 98399, avg loss 0.1468539834022522
step 98499, avg loss 0.07313301414251328
step 98599, avg loss 0.10999812930822372
step 98699, avg loss 0.11731540411710739
step 98799, avg loss 0.1072445809841156
step 98899, avg loss 0.09706350415945053
step 98999, avg loss 0.07442363351583481
step 99099, avg loss 0.15153180062770844
step 99199, avg loss 0.12272263318300247
step 99299, avg loss 0.10649460554122925
step 99399, avg loss 0.09250481426715851
step 99499, avg loss 0.13257068395614624
step 99599, avg loss 0.07757516950368881
step 99699, avg loss 0.12710106372833252
step 99799, avg loss 0.1440734714269638
step 99899, avg loss 0.09517852216959
step 99999, avg loss 0.1320483535528183
saving model at long_biencoder/100000
