nohup: ignoring input
[2023-08-13 04:45:04,443] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Batch size: 32
Starting data loading...
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
/home/shreyansh/long_context_biencoder_v2/mosaic_bert/bert_layers.py:177: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).
  warnings.warn(
Some weights of BertModel were not initialized from the model checkpoint at mosaicml/mosaic-bert-base-seqlen-1024 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
step 499, avg loss 0.5659827589988708
step 999, avg loss 0.24085931479930878
step 1499, avg loss 0.22793902456760406
step 1999, avg loss 0.21984222531318665
step 2499, avg loss 0.21013475954532623
step 2999, avg loss 0.18897540867328644
step 3499, avg loss 0.19502969086170197
step 3999, avg loss 0.18487632274627686
step 4499, avg loss 0.19515572488307953
step 4999, avg loss 0.17083267867565155
step 5499, avg loss 0.20085719227790833
step 5999, avg loss 0.19981665909290314
step 6499, avg loss 0.16846901178359985
step 6999, avg loss 0.202864408493042
step 7499, avg loss 0.18356101214885712
step 7999, avg loss 0.18662823736667633
step 8499, avg loss 0.16801925003528595
step 8999, avg loss 0.18655693531036377
step 9499, avg loss 0.18240711092948914
step 9999, avg loss 0.16778194904327393
step 10499, avg loss 0.1777057945728302
step 10999, avg loss 0.1749582290649414
step 11499, avg loss 0.14849229156970978
step 11999, avg loss 0.17985554039478302
step 12499, avg loss 0.16442951560020447
step 12999, avg loss 0.17160214483737946
step 13499, avg loss 0.17897047102451324
step 13999, avg loss 0.16663295030593872
step 14499, avg loss 0.17483490705490112
step 14999, avg loss 0.15983466804027557
step 15499, avg loss 0.17516328394412994
step 15999, avg loss 0.14662866294384003
step 16499, avg loss 0.18052908778190613
step 16999, avg loss 0.1650325208902359
step 17499, avg loss 0.16436991095542908
step 17999, avg loss 0.16177783906459808
step 18499, avg loss 0.13926337659358978
step 18999, avg loss 0.15236754715442657
step 19499, avg loss 0.1650557667016983
step 19999, avg loss 0.16222991049289703
step 20499, avg loss 0.156642884016037
step 20999, avg loss 0.17743851244449615
step 21499, avg loss 0.1329767107963562
step 21999, avg loss 0.1550668478012085
step 22499, avg loss 0.15471643209457397
step 22999, avg loss 0.15084291994571686
step 23499, avg loss 0.1593029648065567
step 23999, avg loss 0.15396162867546082
step 24499, avg loss 0.1749081164598465
step 24999, avg loss 0.14735493063926697
step 25499, avg loss 0.14046451449394226
step 25999, avg loss 0.14353910088539124
step 26499, avg loss 0.1298070102930069
step 26999, avg loss 0.15353484451770782
step 27499, avg loss 0.13566748797893524
step 27999, avg loss 0.16305416822433472
step 28499, avg loss 0.13644908368587494
step 28999, avg loss 0.1663515418767929
step 29499, avg loss 0.15678878128528595
step 29999, avg loss 0.15798810124397278
step 30499, avg loss 0.1652582585811615
step 30999, avg loss 0.13095451891422272
step 31499, avg loss 0.14921647310256958
step 31999, avg loss 0.12964032590389252
step 32499, avg loss 0.15209276974201202
step 32999, avg loss 0.13867594301700592
step 33499, avg loss 0.1441599428653717
step 33999, avg loss 0.14444246888160706
step 34499, avg loss 0.1368100792169571
step 34999, avg loss 0.13272815942764282
step 35499, avg loss 0.12675128877162933
step 35999, avg loss 0.14070677757263184
step 36499, avg loss 0.14513219892978668
step 36999, avg loss 0.1453254222869873
step 37499, avg loss 0.11929570138454437
step 37999, avg loss 0.14573568105697632
step 38499, avg loss 0.1655161827802658
step 38999, avg loss 0.1409463733434677
step 39499, avg loss 0.1231549084186554
step 39999, avg loss 0.13776814937591553
step 40499, avg loss 0.1450422704219818
step 40999, avg loss 0.1575198471546173
step 41499, avg loss 0.13496606051921844
step 41999, avg loss 0.14355884492397308
step 42499, avg loss 0.13894572854042053
step 42999, avg loss 0.13568492233753204
step 43499, avg loss 0.1319730430841446
step 43999, avg loss 0.144161194562912
step 44499, avg loss 0.14666974544525146
step 44999, avg loss 0.14378075301647186
step 45499, avg loss 0.14197154343128204
step 45999, avg loss 0.1423860490322113
step 46499, avg loss 0.13323822617530823
step 46999, avg loss 0.14076553285121918
step 47499, avg loss 0.1466778963804245
step 47999, avg loss 0.14637210965156555
step 48499, avg loss 0.1335156410932541
step 48999, avg loss 0.13191404938697815
step 49499, avg loss 0.13595202565193176
step 49999, avg loss 0.14118660986423492
saving model at long_biencoder_1m/50000
step 50499, avg loss 0.14366215467453003
step 50999, avg loss 0.12401539087295532
step 51499, avg loss 0.1368188112974167
step 51999, avg loss 0.13105106353759766
step 52499, avg loss 0.1263059824705124
step 52999, avg loss 0.1409154087305069
step 53499, avg loss 0.1462181955575943
step 53999, avg loss 0.15663857758045197
step 54499, avg loss 0.12610292434692383
step 54999, avg loss 0.14570850133895874
step 55499, avg loss 0.13690482079982758
step 55999, avg loss 0.135446235537529
step 56499, avg loss 0.1385342925786972
step 56999, avg loss 0.1223873421549797
step 57499, avg loss 0.1015092208981514
step 57999, avg loss 0.14459653198719025
step 58499, avg loss 0.11854668706655502
step 58999, avg loss 0.12669271230697632
step 59499, avg loss 0.16218096017837524
step 59999, avg loss 0.14329439401626587
step 60499, avg loss 0.14924666285514832
step 60999, avg loss 0.16245225071907043
step 61499, avg loss 0.13353806734085083
step 61999, avg loss 0.14805813133716583
step 62499, avg loss 0.14266397058963776
step 62999, avg loss 0.126517653465271
step 63499, avg loss 0.15405143797397614
step 63999, avg loss 0.11750981956720352
step 64499, avg loss 0.12397067248821259
step 64999, avg loss 0.1410890817642212
step 65499, avg loss 0.13733109831809998
step 65999, avg loss 0.1263425350189209
step 66499, avg loss 0.15278808772563934
step 66999, avg loss 0.14434699714183807
step 67499, avg loss 0.13571541011333466
step 67999, avg loss 0.12674351036548615
step 68499, avg loss 0.13539591431617737
step 68999, avg loss 0.12550555169582367
step 69499, avg loss 0.13820995390415192
step 69999, avg loss 0.120644211769104
step 70499, avg loss 0.1367160826921463
step 70999, avg loss 0.14222432672977448
step 71499, avg loss 0.12420306354761124
step 71999, avg loss 0.12525716423988342
step 72499, avg loss 0.1319466382265091
step 72999, avg loss 0.13375850021839142
step 73499, avg loss 0.1294778436422348
step 73999, avg loss 0.13756872713565826
step 74499, avg loss 0.13570919632911682
step 74999, avg loss 0.1284271627664566
step 75499, avg loss 0.14342263340950012
step 75999, avg loss 0.13691776990890503
step 76499, avg loss 0.14400747418403625
step 76999, avg loss 0.13356685638427734
step 77499, avg loss 0.1176968440413475
step 77999, avg loss 0.12093105167150497
step 78499, avg loss 0.1432722806930542
step 78999, avg loss 0.1216818317770958
step 79499, avg loss 0.13922975957393646
step 79999, avg loss 0.13104934990406036
step 80499, avg loss 0.1316140592098236
step 80999, avg loss 0.12573358416557312
step 81499, avg loss 0.12126561254262924
step 81999, avg loss 0.12242607772350311
step 82499, avg loss 0.13513199985027313
step 82999, avg loss 0.1334162950515747
step 83499, avg loss 0.12866446375846863
step 83999, avg loss 0.1325046569108963
step 84499, avg loss 0.13339704275131226
step 84999, avg loss 0.1312200427055359
step 85499, avg loss 0.12361758947372437
step 85999, avg loss 0.12344493716955185
step 86499, avg loss 0.1341722011566162
step 86999, avg loss 0.12088716775178909
step 87499, avg loss 0.13336892426013947
step 87999, avg loss 0.14222398400306702
step 88499, avg loss 0.1292858123779297
step 88999, avg loss 0.12869715690612793
step 89499, avg loss 0.12244898080825806
step 89999, avg loss 0.12110704183578491
step 90499, avg loss 0.13130950927734375
step 90999, avg loss 0.11297349631786346
step 91499, avg loss 0.13129964470863342
step 91999, avg loss 0.1331288069486618
step 92499, avg loss 0.12521503865718842
step 92999, avg loss 0.112310990691185
step 93499, avg loss 0.13696646690368652
step 93999, avg loss 0.12078104168176651
step 94499, avg loss 0.12392710149288177
step 94999, avg loss 0.13472558557987213
step 95499, avg loss 0.1250428408384323
step 95999, avg loss 0.12326367199420929
step 96499, avg loss 0.12326278537511826
step 96999, avg loss 0.13377629220485687
step 97499, avg loss 0.1298111379146576
step 97999, avg loss 0.11815744638442993
step 98499, avg loss 0.11368745565414429
step 98999, avg loss 0.10509192198514938
step 99499, avg loss 0.12672114372253418
step 99999, avg loss 0.1229730024933815
saving model at long_biencoder_1m/100000
step 100499, avg loss 0.13988158106803894
step 100999, avg loss 0.12197725474834442
step 101499, avg loss 0.12577901780605316
step 101999, avg loss 0.12986867129802704
step 102499, avg loss 0.12377576529979706
step 102999, avg loss 0.10828112810850143
step 103499, avg loss 0.13963466882705688
step 103999, avg loss 0.11538530886173248
step 104499, avg loss 0.12437068670988083
step 104999, avg loss 0.13750839233398438
step 105499, avg loss 0.10301791876554489
step 105999, avg loss 0.12011328339576721
step 106499, avg loss 0.12364201247692108
step 106999, avg loss 0.13467377424240112
step 107499, avg loss 0.12116530537605286
step 107999, avg loss 0.12992462515830994
step 108499, avg loss 0.11416281759738922
step 108999, avg loss 0.12495025247335434
step 109499, avg loss 0.11348378658294678
step 109999, avg loss 0.1348598450422287
step 110499, avg loss 0.11015163362026215
step 110999, avg loss 0.13671086728572845
step 111499, avg loss 0.12324044853448868
step 111999, avg loss 0.10999494791030884
step 112499, avg loss 0.15318824350833893
step 112999, avg loss 0.13150674104690552
step 113499, avg loss 0.1393289715051651
step 113999, avg loss 0.12339402735233307
step 114499, avg loss 0.13986000418663025
step 114999, avg loss 0.10869133472442627
step 115499, avg loss 0.12533599138259888
step 115999, avg loss 0.1224570944905281
step 116499, avg loss 0.12890121340751648
step 116999, avg loss 0.12457013875246048
step 117499, avg loss 0.11835908889770508
step 117999, avg loss 0.13224630057811737
step 118499, avg loss 0.11414580047130585
step 118999, avg loss 0.12234281003475189
step 119499, avg loss 0.12750814855098724
step 119999, avg loss 0.12488831579685211
step 120499, avg loss 0.1313251405954361
step 120999, avg loss 0.1308218389749527
step 121499, avg loss 0.12041790783405304
step 121999, avg loss 0.13510076701641083
step 122499, avg loss 0.1357860118150711
step 122999, avg loss 0.11874904483556747
step 123499, avg loss 0.12756377458572388
step 123999, avg loss 0.11868441849946976
step 124499, avg loss 0.1022033616900444
step 124999, avg loss 0.11969811469316483
step 125499, avg loss 0.1331557184457779
step 125999, avg loss 0.1191670149564743
step 126499, avg loss 0.13691064715385437
step 126999, avg loss 0.10738160461187363
step 127499, avg loss 0.11363905668258667
step 127999, avg loss 0.1166587620973587
step 128499, avg loss 0.11155491322278976
step 128999, avg loss 0.12705805897712708
step 129499, avg loss 0.13213355839252472
step 129999, avg loss 0.11394292861223221
step 130499, avg loss 0.12473868578672409
step 130999, avg loss 0.12006090581417084
step 131499, avg loss 0.10894574970006943
step 131999, avg loss 0.12792018055915833
step 132499, avg loss 0.1220606341958046
step 132999, avg loss 0.1117991954088211
step 133499, avg loss 0.11568278819322586
step 133999, avg loss 0.1432020217180252
step 134499, avg loss 0.11219260841608047
step 134999, avg loss 0.1297602355480194
step 135499, avg loss 0.1399238556623459
step 135999, avg loss 0.10340403765439987
step 136499, avg loss 0.11569757759571075
step 136999, avg loss 0.14970563352108002
step 137499, avg loss 0.12805984914302826
step 137999, avg loss 0.1256485879421234
step 138499, avg loss 0.14313340187072754
step 138999, avg loss 0.09642534703016281
step 139499, avg loss 0.12215463072061539
step 139999, avg loss 0.13061916828155518
step 140499, avg loss 0.11971893161535263
step 140999, avg loss 0.12845447659492493
step 141499, avg loss 0.09746253490447998
step 141999, avg loss 0.1269068866968155
step 142499, avg loss 0.10964736342430115
step 142999, avg loss 0.1149645522236824
step 143499, avg loss 0.11488695442676544
step 143999, avg loss 0.12205711007118225
step 144499, avg loss 0.12624183297157288
step 144999, avg loss 0.12488864362239838
step 145499, avg loss 0.1264033168554306
step 145999, avg loss 0.1217610090970993
step 146499, avg loss 0.11243540793657303
step 146999, avg loss 0.11671853065490723
step 147499, avg loss 0.1172727718949318
step 147999, avg loss 0.10920727252960205
step 148499, avg loss 0.13638867437839508
step 148999, avg loss 0.10213510692119598
step 149499, avg loss 0.10985890030860901
step 149999, avg loss 0.12114988267421722
saving model at long_biencoder_1m/150000
step 150499, avg loss 0.13446828722953796
step 150999, avg loss 0.11482276767492294
step 151499, avg loss 0.1370510309934616
step 151999, avg loss 0.13296757638454437
step 152499, avg loss 0.1381506472826004
step 152999, avg loss 0.13542291522026062
step 153499, avg loss 0.1265355944633484
step 153999, avg loss 0.12992632389068604
step 154499, avg loss 0.12282279133796692
step 154999, avg loss 0.10866605490446091
step 155499, avg loss 0.11371755599975586
step 155999, avg loss 0.1514458805322647
step 156499, avg loss 0.11020732671022415
step 156999, avg loss 0.1172780692577362
step 157499, avg loss 0.11929592490196228
step 157999, avg loss 0.10703496634960175
step 158499, avg loss 0.12140563130378723
step 158999, avg loss 0.1316705197095871
step 159499, avg loss 0.12441039085388184
step 159999, avg loss 0.10419590771198273
step 160499, avg loss 0.114408940076828
step 160999, avg loss 0.10475649684667587
step 161499, avg loss 0.13959766924381256
step 161999, avg loss 0.12908592820167542
step 162499, avg loss 0.1130017638206482
step 162999, avg loss 0.13562580943107605
step 163499, avg loss 0.13570988178253174
step 163999, avg loss 0.12432397902011871
step 164499, avg loss 0.12089639157056808
step 164999, avg loss 0.12001942843198776
step 165499, avg loss 0.10361140966415405
step 165999, avg loss 0.12336396425962448
step 166499, avg loss 0.11868803948163986
step 166999, avg loss 0.11344657838344574
step 167499, avg loss 0.11171731352806091
step 167999, avg loss 0.11967279762029648
step 168499, avg loss 0.1225137785077095
step 168999, avg loss 0.10659843683242798
step 169499, avg loss 0.1204768493771553
step 169999, avg loss 0.11652853339910507
step 170499, avg loss 0.10020395368337631
step 170999, avg loss 0.12454190850257874
step 171499, avg loss 0.10941092669963837
step 171999, avg loss 0.10767453908920288
step 172499, avg loss 0.10337010025978088
step 172999, avg loss 0.13373644649982452
step 173499, avg loss 0.1197877898812294
step 173999, avg loss 0.11279754340648651
step 174499, avg loss 0.10480524599552155
step 174999, avg loss 0.10996957868337631
step 175499, avg loss 0.12430836260318756
step 175999, avg loss 0.1194269135594368
step 176499, avg loss 0.10333521664142609
step 176999, avg loss 0.10278896242380142
step 177499, avg loss 0.1247563287615776
step 177999, avg loss 0.11401324719190598
step 178499, avg loss 0.10776320844888687
step 178999, avg loss 0.11712946742773056
step 179499, avg loss 0.12553605437278748
step 179999, avg loss 0.13147354125976562
step 180499, avg loss 0.12182638794183731
step 180999, avg loss 0.12513892352581024
step 181499, avg loss 0.14356014132499695
step 181999, avg loss 0.12731881439685822
step 182499, avg loss 0.1085447371006012
step 182999, avg loss 0.12046483159065247
step 183499, avg loss 0.12200997769832611
step 183999, avg loss 0.1337471455335617
step 184499, avg loss 0.12525324523448944
step 184999, avg loss 0.10809874534606934
step 185499, avg loss 0.1278151124715805
step 185999, avg loss 0.10897359251976013
step 186499, avg loss 0.1263061761856079
step 186999, avg loss 0.0919976532459259
step 187499, avg loss 0.10857754945755005
step 187999, avg loss 0.1337449848651886
step 188499, avg loss 0.10771160572767258
step 188999, avg loss 0.12277118861675262
step 189499, avg loss 0.11887869983911514
step 189999, avg loss 0.1312505602836609
step 190499, avg loss 0.11978766322135925
step 190999, avg loss 0.10572312772274017
step 191499, avg loss 0.12907035648822784
step 191999, avg loss 0.11838392913341522
step 192499, avg loss 0.13164110481739044
step 192999, avg loss 0.11936947703361511
step 193499, avg loss 0.10718746483325958
step 193999, avg loss 0.1178961992263794
step 194499, avg loss 0.11843633651733398
step 194999, avg loss 0.11434654146432877
step 195499, avg loss 0.1402786523103714
step 195999, avg loss 0.0959065854549408
step 196499, avg loss 0.11869308352470398
step 196999, avg loss 0.10731172561645508
step 197499, avg loss 0.11467836797237396
step 197999, avg loss 0.11231030523777008
step 198499, avg loss 0.11577343940734863
step 198999, avg loss 0.10544057190418243
step 199499, avg loss 0.12283527106046677
step 199999, avg loss 0.116078220307827
saving model at long_biencoder_1m/200000
step 200499, avg loss 0.12488700449466705
step 200999, avg loss 0.12187705188989639
step 201499, avg loss 0.10539155453443527
step 201999, avg loss 0.1181221753358841
step 202499, avg loss 0.12963299453258514
step 202999, avg loss 0.14049142599105835
step 203499, avg loss 0.11672256141901016
step 203999, avg loss 0.12017911672592163
step 204499, avg loss 0.1243770644068718
step 204999, avg loss 0.11794519424438477
step 205499, avg loss 0.10766786336898804
step 205999, avg loss 0.11644516885280609
step 206499, avg loss 0.11867997795343399
step 206999, avg loss 0.12493744492530823
step 207499, avg loss 0.11804991215467453
step 207999, avg loss 0.1435481160879135
step 208499, avg loss 0.11953753978013992
step 208999, avg loss 0.1000315248966217
step 209499, avg loss 0.10482662916183472
step 209999, avg loss 0.11235295236110687
step 210499, avg loss 0.1033565104007721
step 210999, avg loss 0.11380302160978317
step 211499, avg loss 0.10550138354301453
step 211999, avg loss 0.11409127712249756
step 212499, avg loss 0.11393896490335464
step 212999, avg loss 0.1135142520070076
step 213499, avg loss 0.11978015303611755
step 213999, avg loss 0.12110777944326401
step 214499, avg loss 0.11697669327259064
step 214999, avg loss 0.11617715656757355
step 215499, avg loss 0.11661284416913986
step 215999, avg loss 0.12292473018169403
step 216499, avg loss 0.15969166159629822
step 216999, avg loss 0.12672016024589539
step 217499, avg loss 0.10193274915218353
step 217999, avg loss 0.14438693225383759
step 218499, avg loss 0.14477449655532837
step 218999, avg loss 0.13845883309841156
step 219499, avg loss 0.1066281870007515
step 219999, avg loss 0.1304132044315338
step 220499, avg loss 0.14075635373592377
step 220999, avg loss 0.12817716598510742
step 221499, avg loss 0.12389751523733139
step 221999, avg loss 0.1273823231458664
step 222499, avg loss 0.1510796993970871
step 222999, avg loss 0.15112781524658203
step 223499, avg loss 0.11488955467939377
step 223999, avg loss 0.10498527437448502
step 224499, avg loss 0.11851824820041656
step 224999, avg loss 0.12556861340999603
step 225499, avg loss 0.1156938299536705
step 225999, avg loss 0.12651965022087097
step 226499, avg loss 0.13524216413497925
step 226999, avg loss 0.1507362276315689
step 227499, avg loss 0.1296946108341217
step 227999, avg loss 0.13229064643383026
step 228499, avg loss 0.1220407709479332
step 228999, avg loss 0.1278766393661499
step 229499, avg loss 0.12617240846157074
step 229999, avg loss 0.12311669439077377
step 230499, avg loss 0.13401149213314056
step 230999, avg loss 0.11679383367300034
step 231499, avg loss 0.12337607890367508
step 231999, avg loss 0.12680982053279877
step 232499, avg loss 0.11918779462575912
step 232999, avg loss 0.1360047161579132
step 233499, avg loss 0.10495923459529877
step 233999, avg loss 0.09254459291696548
step 234499, avg loss 0.1288609653711319
step 234999, avg loss 0.11078090965747833
step 235499, avg loss 0.13468611240386963
step 235999, avg loss 0.12055355310440063
step 236499, avg loss 0.12092761695384979
step 236999, avg loss 0.12161775678396225
step 237499, avg loss 0.12466998398303986
step 237999, avg loss 0.10761141777038574
step 238499, avg loss 0.11279963701963425
step 238999, avg loss 0.11504687368869781
step 239499, avg loss 0.13471820950508118
step 239999, avg loss 0.11906982213258743
step 240499, avg loss 0.13397113978862762
step 240999, avg loss 0.12099383771419525
step 241499, avg loss 0.13567718863487244
step 241999, avg loss 0.12631915509700775
step 242499, avg loss 0.12377841025590897
step 242999, avg loss 0.12822329998016357
step 243499, avg loss 0.09834969788789749
step 243999, avg loss 0.135126993060112
step 244499, avg loss 0.11706547439098358
step 244999, avg loss 0.10569152981042862
step 245499, avg loss 0.12795956432819366
step 245999, avg loss 0.12175541371107101
step 246499, avg loss 0.13720835745334625
step 246999, avg loss 0.11861220747232437
step 247499, avg loss 0.12015736848115921
step 247999, avg loss 0.12170697748661041
step 248499, avg loss 0.12416478991508484
step 248999, avg loss 0.16521260142326355
step 249499, avg loss 0.11541297286748886
step 249999, avg loss 0.12170996516942978
saving model at long_biencoder_1m/250000
step 250499, avg loss 0.14749139547348022
step 250999, avg loss 0.12784865498542786
step 251499, avg loss 0.12128172069787979
step 251999, avg loss 0.1411132514476776
step 252499, avg loss 0.13559161126613617
step 252999, avg loss 0.11874952167272568
step 253499, avg loss 0.1328159123659134
step 253999, avg loss 0.12605701386928558
step 254499, avg loss 0.1266748607158661
step 254999, avg loss 0.14386583864688873
step 255499, avg loss 0.13471266627311707
step 255999, avg loss 0.11817198991775513
step 256499, avg loss 0.13687323033809662
step 256999, avg loss 0.14418256282806396
step 257499, avg loss 0.11593708395957947
step 257999, avg loss 0.11543340981006622
step 258499, avg loss 0.11750709265470505
step 258999, avg loss 0.11516167968511581
step 259499, avg loss 0.1090119406580925
step 259999, avg loss 0.10670492053031921
step 260499, avg loss 0.11202097684144974
step 260999, avg loss 0.15435124933719635
step 261499, avg loss 0.14044438302516937
step 261999, avg loss 0.12814870476722717
step 262499, avg loss 0.11727732419967651
step 262999, avg loss 0.12719498574733734
step 263499, avg loss 0.1389523446559906
step 263999, avg loss 0.14466045796871185
step 264499, avg loss 0.1142529845237732
step 264999, avg loss 0.11500157415866852
step 265499, avg loss 0.11452590674161911
step 265999, avg loss 0.12738308310508728
step 266499, avg loss 0.12201278656721115
step 266999, avg loss 0.13310378789901733
step 267499, avg loss 0.11946727335453033
step 267999, avg loss 0.13087891042232513
step 268499, avg loss 0.12054112553596497
step 268999, avg loss 0.1120619997382164
step 269499, avg loss 0.1395040601491928
step 269999, avg loss 0.11638474464416504
step 270499, avg loss 0.12163909524679184
step 270999, avg loss 0.10900015383958817
step 271499, avg loss 0.10001741349697113
step 271999, avg loss 0.12447044253349304
step 272499, avg loss 0.13075309991836548
step 272999, avg loss 0.115011066198349
step 273499, avg loss 0.1256759911775589
step 273999, avg loss 0.10307789593935013
step 274499, avg loss 0.11064094305038452
step 274999, avg loss 0.139248788356781
step 275499, avg loss 0.11483236402273178
step 275999, avg loss 0.12024039775133133
step 276499, avg loss 0.14034011960029602
step 276999, avg loss 0.1000884547829628
step 277499, avg loss 0.14295010268688202
step 277999, avg loss 0.12621380388736725
step 278499, avg loss 0.14736920595169067
step 278999, avg loss 0.10894621163606644
step 279499, avg loss 0.10118704289197922
step 279999, avg loss 0.12140332162380219
step 280499, avg loss 0.09414037317037582
step 280999, avg loss 0.12296652793884277
step 281499, avg loss 0.11966799199581146
step 281999, avg loss 0.11962974071502686
step 282499, avg loss 0.11967501789331436
step 282999, avg loss 0.11988872289657593
step 283499, avg loss 0.10995741188526154
step 283999, avg loss 0.10215277224779129
step 284499, avg loss 0.10567762702703476
step 284999, avg loss 0.12674333155155182
step 285499, avg loss 0.13349799811840057
step 285999, avg loss 0.11580589413642883
step 286499, avg loss 0.12690332531929016
step 286999, avg loss 0.11213263869285583
step 287499, avg loss 0.12781992554664612
step 287999, avg loss 0.10244929790496826
step 288499, avg loss 0.1453332006931305
step 288999, avg loss 0.13410523533821106
step 289499, avg loss 0.11471707373857498
step 289999, avg loss 0.10843466967344284
step 290499, avg loss 0.12778764963150024
step 290999, avg loss 0.13134074211120605
step 291499, avg loss 0.13859662413597107
step 291999, avg loss 0.11735222488641739
step 292499, avg loss 0.13284845650196075
step 292999, avg loss 0.1231296956539154
step 293499, avg loss 0.12012004107236862
step 293999, avg loss 0.11643222719430923
step 294499, avg loss 0.14931222796440125
step 294999, avg loss 0.12630930542945862
step 295499, avg loss 0.10009964555501938
step 295999, avg loss 0.13367938995361328
step 296499, avg loss 0.12104726582765579
step 296999, avg loss 0.10626339167356491
step 297499, avg loss 0.09156954288482666
step 297999, avg loss 0.11433765292167664
step 298499, avg loss 0.1260959357023239
step 298999, avg loss 0.12506067752838135
step 299499, avg loss 0.09966053813695908
step 299999, avg loss 0.1132674515247345
saving model at long_biencoder_1m/300000
step 300499, avg loss 0.12766094505786896
step 300999, avg loss 0.12399604171514511
step 301499, avg loss 0.12035123258829117
step 301999, avg loss 0.11947756260633469
step 302499, avg loss 0.13015145063400269
step 302999, avg loss 0.11110062897205353
step 303499, avg loss 0.10592115670442581
step 303999, avg loss 0.11704858392477036
step 304499, avg loss 0.10413754731416702
step 304999, avg loss 0.10336734354496002
step 305499, avg loss 0.1219855472445488
step 305999, avg loss 0.11731711775064468
step 306499, avg loss 0.12616842985153198
step 306999, avg loss 0.1366536170244217
step 307499, avg loss 0.12631911039352417
step 307999, avg loss 0.1109328642487526
step 308499, avg loss 0.11287517845630646
step 308999, avg loss 0.11883443593978882
step 309499, avg loss 0.14047999680042267
step 309999, avg loss 0.11288625746965408
step 310499, avg loss 0.10062605142593384
step 310999, avg loss 0.14951322972774506
step 311499, avg loss 0.11994468420743942
step 311999, avg loss 0.11300614476203918
step 312499, avg loss 0.12406405806541443
step 312999, avg loss 0.13918384909629822
step 313499, avg loss 0.12382141500711441
step 313999, avg loss 0.11501171439886093
step 314499, avg loss 0.13588659465312958
step 314999, avg loss 0.10914761573076248
step 315499, avg loss 0.13535603880882263
step 315999, avg loss 0.13610152900218964
step 316499, avg loss 0.1165805235505104
step 316999, avg loss 0.12755340337753296
step 317499, avg loss 0.14022119343280792
step 317999, avg loss 0.1218252182006836
step 318499, avg loss 0.12948361039161682
step 318999, avg loss 0.10664867609739304
step 319499, avg loss 0.11047708243131638
step 319999, avg loss 0.10412713140249252
step 320499, avg loss 0.10212931782007217
step 320999, avg loss 0.12664443254470825
step 321499, avg loss 0.12253863364458084
step 321999, avg loss 0.11799539625644684
step 322499, avg loss 0.11693219840526581
step 322999, avg loss 0.11194808036088943
step 323499, avg loss 0.10172265768051147
step 323999, avg loss 0.11828187853097916
step 324499, avg loss 0.11449069529771805
step 324999, avg loss 0.1325959712266922
step 325499, avg loss 0.12937031686306
step 325999, avg loss 0.12226026505231857
step 326499, avg loss 0.11522701382637024
step 326999, avg loss 0.10610079765319824
step 327499, avg loss 0.11048980802297592
step 327999, avg loss 0.12463023513555527
step 328499, avg loss 0.10513074696063995
step 328999, avg loss 0.14724808931350708
step 329499, avg loss 0.10864640772342682
step 329999, avg loss 0.10772830992937088
step 330499, avg loss 0.13486430048942566
step 330999, avg loss 0.10409330576658249
step 331499, avg loss 0.13450263440608978
step 331999, avg loss 0.12122282385826111
step 332499, avg loss 0.11256231367588043
step 332999, avg loss 0.10651898384094238
step 333499, avg loss 0.14140522480010986
step 333999, avg loss 0.12495634704828262
step 334499, avg loss 0.105007603764534
step 334999, avg loss 0.12319139391183853
step 335499, avg loss 0.10321147739887238
step 335999, avg loss 0.1257379949092865
step 336499, avg loss 0.12729965150356293
step 336999, avg loss 0.11142879724502563
step 337499, avg loss 0.11693378537893295
step 337999, avg loss 0.155060812830925
step 338499, avg loss 0.10200811922550201
step 338999, avg loss 0.10658270120620728
step 339499, avg loss 0.11784670501947403
step 339999, avg loss 0.12314407527446747
step 340499, avg loss 0.11803553253412247
step 340999, avg loss 0.14260156452655792
step 341499, avg loss 0.12319809198379517
step 341999, avg loss 0.14444978535175323
step 342499, avg loss 0.1414237767457962
step 342999, avg loss 0.1025967001914978
step 343499, avg loss 0.1249028742313385
step 343999, avg loss 0.11665576696395874
step 344499, avg loss 0.1155204325914383
step 344999, avg loss 0.10887227952480316
step 345499, avg loss 0.10783515870571136
step 345999, avg loss 0.11566247045993805
step 346499, avg loss 0.1208377256989479
step 346999, avg loss 0.12583081424236298
step 347499, avg loss 0.11235439777374268
step 347999, avg loss 0.12756122648715973
step 348499, avg loss 0.10907439142465591
step 348999, avg loss 0.15499792993068695
step 349499, avg loss 0.11509592831134796
step 349999, avg loss 0.11135084182024002
saving model at long_biencoder_1m/350000
step 350499, avg loss 0.11854971200227737
step 350999, avg loss 0.1216069906949997
step 351499, avg loss 0.10830369591712952
step 351999, avg loss 0.12525004148483276
step 352499, avg loss 0.1299220472574234
step 352999, avg loss 0.11903834342956543
step 353499, avg loss 0.11934658139944077
step 353999, avg loss 0.09736745804548264
step 354499, avg loss 0.10730546712875366
step 354999, avg loss 0.12027645856142044
step 355499, avg loss 0.1144217997789383
step 355999, avg loss 0.1362561732530594
step 356499, avg loss 0.12574395537376404
step 356999, avg loss 0.11670757085084915
step 357499, avg loss 0.12486742436885834
step 357999, avg loss 0.09574030339717865
step 358499, avg loss 0.1409134566783905
step 358999, avg loss 0.15633592009544373
step 359499, avg loss 0.10762234032154083
step 359999, avg loss 0.12302041798830032
step 360499, avg loss 0.15025995671749115
step 360999, avg loss 0.1060713678598404
step 361499, avg loss 0.12487906962633133
step 361999, avg loss 0.11266850680112839
step 362499, avg loss 0.11602084338665009
step 362999, avg loss 0.11735110729932785
step 363499, avg loss 0.11942365020513535
step 363999, avg loss 0.1439824104309082
step 364499, avg loss 0.1248783990740776
step 364999, avg loss 0.1303805112838745
step 365499, avg loss 0.11707261204719543
step 365999, avg loss 0.12622302770614624
step 366499, avg loss 0.1277279108762741
step 366999, avg loss 0.10102581232786179
step 367499, avg loss 0.12387599050998688
step 367999, avg loss 0.09871697425842285
step 368499, avg loss 0.11491824686527252
step 368999, avg loss 0.11352822184562683
step 369499, avg loss 0.11925359070301056
step 369999, avg loss 0.14431311190128326
step 370499, avg loss 0.11207807064056396
step 370999, avg loss 0.12479361146688461
step 371499, avg loss 0.11461309343576431
step 371999, avg loss 0.10150522738695145
step 372499, avg loss 0.1071370393037796
step 372999, avg loss 0.09310929477214813
step 373499, avg loss 0.11361341178417206
step 373999, avg loss 0.16851185262203217
step 374499, avg loss 0.11815691739320755
step 374999, avg loss 0.1172463595867157
step 375499, avg loss 0.13357052206993103
step 375999, avg loss 0.1122925728559494
step 376499, avg loss 0.12459550052881241
step 376999, avg loss 0.12231025099754333
step 377499, avg loss 0.11878695338964462
step 377999, avg loss 0.10083512961864471
step 378499, avg loss 0.14131729304790497
step 378999, avg loss 0.11601012945175171
step 379499, avg loss 0.12209048867225647
step 379999, avg loss 0.13257156312465668
step 380499, avg loss 0.12631356716156006
step 380999, avg loss 0.13283978402614594
step 381499, avg loss 0.12911976873874664
step 381999, avg loss 0.10912276059389114
step 382499, avg loss 0.11670318245887756
step 382999, avg loss 0.12181225419044495
step 383499, avg loss 0.12495139986276627
step 383999, avg loss 0.12187924236059189
step 384499, avg loss 0.10447132587432861
step 384999, avg loss 0.13654306530952454
step 385499, avg loss 0.12800003588199615
step 385999, avg loss 0.10653500258922577
step 386499, avg loss 0.12964239716529846
step 386999, avg loss 0.13313913345336914
step 387499, avg loss 0.12139587849378586
step 387999, avg loss 0.11291225999593735
step 388499, avg loss 0.12419559806585312
step 388999, avg loss 0.11640462279319763
step 389499, avg loss 0.13160040974617004
step 389999, avg loss 0.11986914277076721
step 390499, avg loss 0.12593857944011688
step 390999, avg loss 0.1312435120344162
step 391499, avg loss 0.12448073923587799
step 391999, avg loss 0.1278950423002243
step 392499, avg loss 0.1279980093240738
step 392999, avg loss 0.1120401993393898
step 393499, avg loss 0.11073578149080276
step 393999, avg loss 0.09178521484136581
step 394499, avg loss 0.13495413959026337
step 394999, avg loss 0.12335411459207535
step 395499, avg loss 0.12197675555944443
step 395999, avg loss 0.10119830816984177
step 396499, avg loss 0.11887404322624207
step 396999, avg loss 0.10592823475599289
step 397499, avg loss 0.14417211711406708
step 397999, avg loss 0.11759331822395325
step 398499, avg loss 0.120261050760746
step 398999, avg loss 0.12650112807750702
step 399499, avg loss 0.11376869678497314
step 399999, avg loss 0.11584454774856567
saving model at long_biencoder_1m/400000
step 400499, avg loss 0.12013900279998779
step 400999, avg loss 0.11341285705566406
step 401499, avg loss 0.12950851023197174
step 401999, avg loss 0.12010140717029572
step 402499, avg loss 0.1331353783607483
step 402999, avg loss 0.12626075744628906
step 403499, avg loss 0.13075000047683716
step 403999, avg loss 0.11041957139968872
step 404499, avg loss 0.13251303136348724
step 404999, avg loss 0.11982087045907974
step 405499, avg loss 0.11398546397686005
step 405999, avg loss 0.12010041624307632
step 406499, avg loss 0.11697861552238464
step 406999, avg loss 0.1180562824010849
step 407499, avg loss 0.11152270436286926
step 407999, avg loss 0.11202773451805115
step 408499, avg loss 0.11323808133602142
step 408999, avg loss 0.11431846767663956
step 409499, avg loss 0.12053561210632324
step 409999, avg loss 0.10021355748176575
step 410499, avg loss 0.13270018994808197
step 410999, avg loss 0.11348681151866913
step 411499, avg loss 0.10907584428787231
step 411999, avg loss 0.09183651208877563
step 412499, avg loss 0.1113065630197525
step 412999, avg loss 0.10406912863254547
step 413499, avg loss 0.08326894044876099
step 413999, avg loss 0.10727553814649582
step 414499, avg loss 0.14857694506645203
step 414999, avg loss 0.10588676482439041
step 415499, avg loss 0.11491904407739639
step 415999, avg loss 0.11111503094434738
step 416499, avg loss 0.09748101234436035
step 416999, avg loss 0.09304317086935043
step 417499, avg loss 0.12669889628887177
step 417999, avg loss 0.09359654784202576
step 418499, avg loss 0.12834805250167847
step 418999, avg loss 0.12589789927005768
step 419499, avg loss 0.11444414407014847
step 419999, avg loss 0.12612028419971466
step 420499, avg loss 0.10592842102050781
step 420999, avg loss 0.10228702425956726
step 421499, avg loss 0.12023825943470001
step 421999, avg loss 0.11509231477975845
step 422499, avg loss 0.13081234693527222
step 422999, avg loss 0.11417289823293686
step 423499, avg loss 0.13117089867591858
step 423999, avg loss 0.10874200612306595
step 424499, avg loss 0.12449350208044052
step 424999, avg loss 0.09838373214006424
step 425499, avg loss 0.08678603917360306
step 425999, avg loss 0.1258092075586319
step 426499, avg loss 0.11665383726358414
step 426999, avg loss 0.119941346347332
step 427499, avg loss 0.12101548165082932
step 427999, avg loss 0.13029135763645172
step 428499, avg loss 0.1274838000535965
step 428999, avg loss 0.12746210396289825
step 429499, avg loss 0.10462487488985062
step 429999, avg loss 0.10326739400625229
step 430499, avg loss 0.13540014624595642
step 430999, avg loss 0.11891064792871475
step 431499, avg loss 0.1212545782327652
step 431999, avg loss 0.11707483977079391
step 432499, avg loss 0.10776376724243164
step 432999, avg loss 0.11754955351352692
step 433499, avg loss 0.1391362100839615
step 433999, avg loss 0.11714256554841995
step 434499, avg loss 0.09909979999065399
step 434999, avg loss 0.09634420275688171
step 435499, avg loss 0.11723092198371887
step 435999, avg loss 0.11057040840387344
step 436499, avg loss 0.10427707433700562
step 436999, avg loss 0.10471635311841965
step 437499, avg loss 0.16674649715423584
step 437999, avg loss 0.10548781603574753
step 438499, avg loss 0.13220646977424622
step 438999, avg loss 0.13066168129444122
step 439499, avg loss 0.10133954882621765
step 439999, avg loss 0.14149416983127594
step 440499, avg loss 0.10950182378292084
step 440999, avg loss 0.11264637857675552
step 441499, avg loss 0.11561170965433121
step 441999, avg loss 0.11772382259368896
step 442499, avg loss 0.09726674854755402
step 442999, avg loss 0.11600083857774734
step 443499, avg loss 0.14085131883621216
step 443999, avg loss 0.1290692687034607
step 444499, avg loss 0.12121015042066574
step 444999, avg loss 0.1337127387523651
step 445499, avg loss 0.10973768681287766
step 445999, avg loss 0.11990547925233841
step 446499, avg loss 0.11996296048164368
step 446999, avg loss 0.11319095641374588
step 447499, avg loss 0.11743224412202835
step 447999, avg loss 0.11683694273233414
step 448499, avg loss 0.12837889790534973
step 448999, avg loss 0.10259245336055756
step 449499, avg loss 0.11062648892402649
step 449999, avg loss 0.10736140608787537
saving model at long_biencoder_1m/450000
step 450499, avg loss 0.11302013695240021
